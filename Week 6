Week 6







=====================================================================
Section 14 - Hashing - The Basics
=====================================================================






/////////////////////////////////////////////////////////////////////
14.1 - Hash Tables - Applications and Operations
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Purpose: maintain evolving set of stuff
Insert: add new record
Delete: delete existing record
Lookup: check for a record
also called a "dictionary"
Guarantee: all these ops run in O(1) time



Application: De-Duplication
=======================================
Given: "stream" of objects (linear scan through huge file, or objects arriving
						    in real time)
Goal: remove duplicates (keep track of unique objects)
Solution: look up each object in hash table. If not found, insert into table.






The 2-SUM Problem
=======================================
Input: unsorted array of n integers. Target sum t.
Goal: whether there are two numbers x,y with x + y = t.
Naive solution: O(n^2) time via exhaustive search
Better: 1. sort A (O(n log n))
		2. for each x in A, look for t-x in A via binary search (O(n log n))
Even better:
	(look for work where there's lots of lookups)
	1. insert elements of A into hash table H  [O(n)]
	2. for each x in A, lookup t-x in H		   [O(1)]






Other applications
=======================================
- symbol tables in compilers
- blocking network traffic
- search algorithms (e.g. game tree exploration)
	- use hash table to avoid exploring any configuration more than once


	










/////////////////////////////////////////////////////////////////////
14.2 - Hash Tables Implementation - Part I
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Setup: universe U (all possible options; generally, REALLY big: all names,
				   all chess board configurations, etc.)
Goal: maintain evolving set S <= U

Naive solutions:
	1. array-based. O(1) ops, but O(U) space
	2. list-based. O(S) space, but O(S) lookup

Our solution:
	1. pick n = # of "buckets" ~= |S|
	2. choose hash function h: U -> {0,1,2,...,n-1} bucket number
	3. use array A of length n, store x in A[h(x)]



Collisions
=======================================

(Sidenote)
It takes only 23 people with random birthdays before you have 50% chance of
same birthday. 
	367 <-- 100%
	57 <-- 99%
	184 <-- 99.99%
"Birthday 'paradox'"  With calendar of k days, you need sqrt(k) to get 50% chance
(/Sidenote)



Collision: distinct x,y in U such that h(x) = h(y)

Solution #1: (separate) chaining. Keep linked list in each bucket
	- given key x, perform ops in the list in A[h(x)]

Solution #2: open addressing. (only one object per bucket)
	- hash function now specifices "probe sequence" h_1(x), h_2(x),...
	  keep trying til find open slot
	- example: linear probing (lookup consecutively)
			   double hashing (use 2 hash functions, where #2 is the shift)


Comparison:
	- if space is premium, use open addressing
	- deletion trickier with open addressing
	- or... just implement both and see how they perform










/////////////////////////////////////////////////////////////////////
14.3 - Hash Tables Implementation - Part II
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

O(list length) could be from m/n to m for m objects for Insert/Delete in chaining
What makes good hash function:
	1. spread data uniformly: "gold standard" = completely random hashing
	2. evaluating hash function runs in constant time
	3. deterministic. Same key = same hash

Bad hash functions for phone numbers:
	- terrible: h(x) = 1st 3 digits of x
	- mediocre: h(x) = last 3 digits of x

Bad hash functions for keys = memory locations (mult of 2):
	- bad: h(x) = x % 1000 => all odd buckets guaranteed to be empty


Quick-and-Dirty Hash Functions
=======================================
(objects U) ----> (integers)    ---->    (buckets)
		(hash code)		(compression function)

Hash code - e.g. conversion strings to integers
Compression function - e.g. the mod n function
How to choose n = # of buckets:
	1. choose n to be a prime (within constant factor of # of objects in table)
			- avoids pathological outputs of mod function
	2. not too close to power of 2
	3. not too close to power of 10






















=====================================================================
Section 15 - Universal Hashing
=====================================================================






/////////////////////////////////////////////////////////////////////
15.1 - Pathological Data Sets
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Load factor
=======================================
alpha = # objects in hash table / # of buckets in hash table
Note: 1. only chaining works for hash tables w/ load factors > 1.
	  2. alpha = O(1) is necessary condition for ops to run in const time
	  3. with open addressing, need alpha << 1
Problem: perfect hash function does not exist. For every hash function, there
	is a pathological data set. 
Reason: fix hash function h, assume |U| >> n
	=> a la Pigeonhole Principle, t.e. bucket i s.t. at least |U|/n elements
	of U hash to i under h, because of compression.



Pathological data in the real world
=======================================
Reference: Crosby and Wallach, USENIX 2003.
Main point: can paralyze several real-world systems (e.g. network intrusion
		detection) by exploiting badly designed hash functions
	- open source
	- overly simplistic hash function
	- easy to reverse engineer pathological data set




Solutions
=======================================
1. use cryptographic hash function (e.g. SHA-2)
	- infeasible to reverse engineer a pathological data set
2. use randomization
	- design family H of hash functions s.t. f.a. data sets S, "almost all"
	  functions h in H spread S out "pretty evenly"
	  (similar to QS guarantee)










/////////////////////////////////////////////////////////////////////
15.2 -- optional
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\






























=====================================================================
Section 16 - Bloom Filters
=====================================================================




/////////////////////////////////////////////////////////////////////
16.1 - Bloom Filters - The Basics
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Supported operations
=======================================
Reason: fast Inserts and Lookups
Comparison to HashTables:
	Pros:
		- more space efficient
	Cons:
		- can't store an associated object
		- no deletions (in vanilla versions)
		- small false positive probability (can mistakenly say you have inserted
											something you haven't)





Applications
=======================================
Original: early spellcheckers
Canonical: list of forbidden passwords
Modern: network routers
		- limited memory, need to be super-fast
		- keep track of blocked IPs, etc








Under the hood
=======================================
Ingredients
	1. array of n bits (so n / |S| = # of bits per object in data set S)
	2. k hash functions h_1,...,h_k (k = small constant)

Insert(x):
	for i = 1,2,...,k
		set A[h_i(x)] = 1		// regardless of previous bit value

Lookup(x):
	return TRUE <=> A[h_i(x)] = 1 for every i = 1,2,...,k
Note: no false negatives. Bits never reset to 0.
Note: some false positives. Chance that combination of other keys caused the
	  bits of this key to be 1.












/////////////////////////////////////////////////////////////////////
16.2 - Heuristic Analysis
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

Fundamental tradeoff: space for bits v. likelihood of error
Assume: [unjustified] all h_i(x)'s uniformly random and independent across
		different i's and x's
Setup: n bits, insert data set S into bloom filter
Note: for each bit of A, probability it's set to 1 is:  1-(1-1/n)^(k|S|)
Note: Probability bit = 0 => (1-1/n)^(k|S|)
So probability bit = 1 => 1 - Pr=0
Chance of surviving single pass: 1-1/n
Number of passes/darts: k*|S|

Recall 1+x <= e^x


1-(1-1/n)^(k|S|)	~= 1 - e^(-k|S|/n) 
					= 1 - e^(-k/b) where b = bits per object
Thus, more bits you have, the closer you get to 1, thus probability goes to 0


[Pr bit=1] ~= 1 - e^(-k/b)
So, under assumption, for x not in S, false positive probability is
	<= [1 - e^(-k/b)]^k
		  ^-- error rate epsilon

How to set k?
	for fixed b, epsilon is minimized by setting k ~= (ln 2) * b
												   ~= 0.693  * b

Plugging back in:
	epsilon ~= (1/2)^(ln 2 * b)  => exponentially small in b
	or...
	b ~= 1.44 log(base 2)(1/epsilon)

Ex: with b = 8, choose k = 5 or 6, error ~= 2%






































Week 2



=====================================================================
Section 4 - The Master Method
=====================================================================


/////////////////////////////////////////////////////////////////////
4.1 - Motivation
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


T(n) = max number ops algorithm needs
Express T(n) in terms of running time of recursive calls

Recall integer multiplication

Base case: T(1) <= a constant
For all n > 1: T(n) <= 4T(n/2) + O(n) 
               (recursive work)  (work here)

Algorithm #2 (Gauss): recursively compute ac, bd, (a+b)(c+d)
recall ad + bc = (3) - (1) - (2)

New recurrence: 
	Base case: T(1) <= a constant
	For n > 1: T(n) <= 3T(n/2) + O(n)
	



/////////////////////////////////////////////////////////////////////
4.2 - Formal statement
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Cool feature: a "black box" for solving recurrences.
Assumption: all subproblems have equal size


Recurrence format
=======================================

1) Base case: T(n) <= a constant for sufficiently small n
2) For all larger n: 
		T(n) <= a*T(n/b) + O(n^d)
		
		where a = number of recursive calls ( >=1)
		      b = factor by which input size shrinks ( >1)
		      d = exponent in running time of "combine step" ( >= 0)
			[a,b,d independent of n]




The Master Method
=======================================

Upper bounded by...

T(n) = {	O(n^d * log(n))		if a = b^d  Case 1
			O(n^d)				if a < b^d  Case 2
			O(n^log(base b)(a))	if a > b^d  Case 3


In case 1, log base doesn't matter. log(base e) and log(base 2) differ
by constant factor.

In case 3, log base matters because it's in the exponent.

In case 2, running time governed by work done outside recursive calls



	
/////////////////////////////////////////////////////////////////////
4.3 - Examples
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Example 1: MergeSort
=======================================

a = 2
b = 2
d = 1

Thus, Case 1: a = b^d
Thus, T(n) <= O(n^d * log(n)) = O(nlog(n))



Example 2: Binary search in sorted array
=======================================

a = 1
b = 2
d = 0

Thus, Case 1:  T(n) <= O(log(n))




Example 3: Integer Multiplication (4 subproblems)
=================================================

a = 4
b = 2
d = 1   <-- additions, padding by 0, thus linear time

Case 3:  T(n) <= O(n^log(base b)(a)) = O(n^log(base 2)(4)) = O(n^2)




Example 4: Integer Multiplication (Gauss's trick)
=================================================

a = 3
b = 2
d = 1

Case 3: T(n) <= O(n^log(base 2)(3)) = O(n^1.59)
Definitely better than n^2...




Example 5: Strassen's Matrix Multiplication
===========================================

Naive approach leads to 8 subproblems
With Strassen's, you get 7.

a = 7
b = 2
d = 2 <-- linear in matrix size, quadratic number of entries, n^2

Case 3: T(n) <= O(n^log(base 2)(7)) = O(n^2.81)
That's a win compared with n^3




Example 6: Ficticious recurrence
=======================================

T(n) <= 2T(n/2) + O(n^2)

=>  a = 2
	b = 2
	d = 2

Case 2:  T(n) <= O(n^2)





/////////////////////////////////////////////////////////////////////
4.4 - Proof of Master Method
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Preamble
=======================================

Assume: recurrence is
			i. T(1) <= c
			ii. T(n) <= aT(n/b) + cn^d
			for some constant c
		and...
			n is a power of b
			(general case similar, but more tedious)



Analysis (similar to MergeSort) using recursion tree
====================================================

At level j = 0,1,...log(base b)(n), 
	there are a^j subproblems each of size n/(b^j)
	
Recursion tree:

	level 0				(input size n)
					/        |            \  (a branches)
	level 1      (n/b)      (n/b)       (n/b)
						...
	level logb(n)	......................... base cases size 1

Work at level j [ignoring work in recursive calls]:
	<= a^j              *     c * [n/(b^j)]^d  
	  # of lvl-j subproblems     size of each level-j subproblem
							 work per level-j subproblem (by ii. in assumption)

	=  cn^d * [a/(b^d)]^j


Total work:
	Summing over all levels
	total work <= cn^d * sum(j=0,log(base b)(n))[a/(b^d)]^j
		call it (*)



/////////////////////////////////////////////////////////////////////
4.5 - Interpretation for 3 Cases
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Interpretation:
	ratio of a/b^d is key to running time
	
	a = rate of subproblem proliferation (RSP)
	b^d = rate of work shrinkage per subproblem (RWS)
	

Tug of war between forces of good (RWS) and evil (RSP)!


Statements:
	- If RSP < RWS, then amount of work is descreasing with recur level j
			True.
	- If RSP > RWS, then amount of work is increasing with recur level j
			True.
	- No conclusions can be drawn about how amount of work varies with recur
	  level j unless RSP and RWS are equal
	  		False, given i and ii
	- If RSP and RWS are equal, amount of work is same at every recur level j



Intuition for 3 cases
=======================================

Upper bound for level j:  cn^d * (a/b^d)^j

1) RSP = RWS => same amount of work each level
	[expect O(n^d*log(n))]

2) RSP < RWS => less work each level => most work at root
	[might expect O(n^d)]

3) RSP > RWS => more work each level => most work at leaves
	[might expect O(# leaves)]


Case 3: how do we get n^log(base b)(a)?





/////////////////////////////////////////////////////////////////////
4.6 - Proof II
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Total work <= cn^d * sum(j=0,log(base b)(n))[ (a/b^d)^j ]  (*)

Case 1, a = b^d:
	(a/b^d)^j reduces to 1
	1 summed to log(base b)(n) times = log(base b)(n) + 1

	(*) = cn^d(log(base b)(n) + 1)
		= O(n^d log(n))

[end case 1]



Basic Sums Fact
=======================================

For r != 1, we have 
	1 + r + r^2 + r^3 +...+ r^k = (r^(k+1) - 1)/(r - 1)

Proof: by induction (you check)
Check out canonical cases of r = 1/2, r = 2

Upshot:
	1) if r < 1 is constant, bounded by <= 1 / (1-r) = a constant
	   independent of k. 1st term of sum dominates.
	2) if r > 1 is constant, RHS is <= r^k * (1 + 1/(r-1))
	   last term of sum dominates



Case 2
=======================================

If a < b^d...

then ratio a/b^d := r < 1   =>  <= a constant independent of n
thus (*) evals to...
	= O(n^d)

total work dominated by top level



Case 3
=======================================

If a > b^d...

then ratio a/b^d := r > 1 =>  <= constant * largest term of sum
take biggest term of sum, which is log(base b)(n)

then (*) = O(n^d * (a/b^d)^log(base b)(n))

Note: b^(-d*log(base b)(n)) = (b^log(base b)n)^-d = n^-d
So: (*) = O(a ^ log(base b)n) , which is number of leaves of recursion tree

We get a children each level, goes on until n reaches 1 by dividing by b,
or log(base b)n

Believe it or not...
	a^log(base b)n  = n^log(base b)a
	to show, take log(base b) of both sides.
	[since (log(base b)n)(log(base b)a) = (log(base b)a)(log(base b)n)]
	
	LHS is more intuitive, RHS is simpler to apply.

[end case 3]

QED!




=====================================================================
Section 5 - Quicksort - Algorithm
=====================================================================



/////////////////////////////////////////////////////////////////////
5.1 - Quicksort overview
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


 Input: array of n numbers, unsorted [3 8 2 5 1 4 7 6]
 Output: same numbers, sorted in increasing order
 Assume: all entries distinct
 Exercise: extend QuickSort to handle duplicate entries
 
 
Partitioning around a pivot
======================================
 
Key idea: partition array around pivot element
- pick element of array
- rearrange element so:
	left of pivot => less than pivot
	right of pivot => greater than pivot
Note: puts pivot in its "rightful position"


Two cool facts about partition
=======================================
1) linear time, no extra memory (only doing swaps)
2) reduces problem size 


High-level description
=======================================

QuickSort(array A, length n) {
	if n = 1 return
	p = choosePivot(A,n)
	partition A around p
	recursively sort 1st part
	recursively sort 2nd part
}





/////////////////////////////////////////////////////////////////////
5.2 - Partition Subroutine
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


In-Place Implementation
=======================================

Assume: pivot = 1st element of array.
Could swap pivot element to be 1st and still use this implementation

High-level idea:
	[p][ already partitioned ][unpartitioned]
	- single scan through array
	- invariant: everything we've looked at is partitioned

Two boundaries: between partitioned and unpartitioned (j)
			    between < p and > p (i)

At each step, we want to advance j.
When you encounter element < pivot, swap with element after i, and advance i
Terminate with swapping pivot and element to the left of i



Pseudocode
=======================================

partition(A, l, r) {
	p = A[l]
	i = l+1
	for j = l+1 to r
		if A[j] < p   // if A[j] > p, do nothing
			swap A[j] and A[i]  // doesn't avoid redundant swaps when no elements
			                    // exist bigger than the pivot
			i++
	swap A[l] and A[i-1]
}



Running time
=======================================

T(n) <= O(n), where n = r - l + 1 is length of input (sub)array
Reason: O(1) work per array entry
Also: clearly works in place (repeated swaps)




Correctness
=======================================

Claim: the for loop maintains the invariant: [p][ < p ][ > p][ ?    ]

1) A[l+1],...,A[i-1] are all less than pivot
2) A[i],...,A[j-1] are all greater than pivot
Exercise: check this by induction

Upshot: at end of for loop and you swap the pivot, the pivot is
"in rightful place"

QED






/////////////////////////////////////////////////////////////////////
5.3 - Correctness of Quicksort [optional]
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Induction Review
=======================================

Let P(n) = assertion parameterized by positive integers n.

For us: 
	P(n) is "QuickSort correctly sorts every input array of length n"

How to prove P(n) for all n >= 1 by induction:
	1) [base case] directly prove the P(1) holds
	2) [inductive step] for every n >= 2, prove that:
		if P(k) holds for all k < n, then P(n) holds as well
		|-inductive hypothesis ----|




Correctness of QuickSort
=======================================

P(n) = "QuickSort correctly sorts every input array of length n"

Claim: 
	P(n) holds for every n >= 1, regardless of pivot selection

Proof by induction:
	1) [base case] every input array of length 1 is already sorted
		QuickSort returns the input array, which is correct. ( P(1) holds)
	2) [inductive step] Fix n >= 2. Fix some input array A of length n.
		Show: if P(k) holds for all k < n, then P(n) holds as well
		
Recall:
	QuickSort partitions A around some pivot p.  [ < p ][ p ][ > p ]
Note:
	pivot winds up in correct position.

Let k_1, k_2 = lengths of 1st, 2nd parts of partitioned array.
	k_1, k_2 < n

By inductive hypothesis: 1st, 2nd parts get sorted correctly by recursive calls.
	(using P(k_1), P(k_2), which are correct by inductive hypothesis)

So: after recursive calls, entire array correctly sorted.





/////////////////////////////////////////////////////////////////////
5.4 - Choosing a Good Pivot
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Running time of QuickSort depends on "quality of pivot" -- how well does it
roughly divide the problem into two equal subproblems

E.g. if you choose first element of sorted array, you get TH(n^2)
Reason:
	Partition subroutine: O(n)
	Subcalls is on zero elements and n-1 elements, so you make n calls
	>= n + (n-1) + (n-2) +...+ 1 = n^2


Suppose we always get the median element as pivot.
Runtime is TH(n log(n))
Reason:
	Let T(n) = running time on arrays of size n
Then:
	T(n) <= 2T(n/2) + O(n) => master method => O(n log n)
	In this case, O(n) is actually TH(n), so we can call it TH(n log n)
	because you can strengthen such statements in the Master Method



Random pivots
=======================================
Big idea: choose random pivots!
Randomized algorithms... Boom.

Hope: a random pivot is "pretty good" "often enough"
1) Intuition: if we always get a 25-75 split, good enough for O(n log n)
Exercise: prove this via recursion tree

2) half of elements give a 25-75 split or better (for 1-100, [26,75])





Average running time of QuickSort
=======================================
The average running time of QuickSort with random pivots is O(n log n)
for every input array of length n
- "average" is over random choices made by algorithm






=====================================================================
Section 6 - Quicksort - Analysis
=====================================================================



/////////////////////////////////////////////////////////////////////
6.1 - Analysis I - A Decomposition Principle
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

We need: finite sample spaces, random variables, expectation, 
		 linearity of expectation



Preliminaries
=======================================

Sample space Omega = all possible outcomes of random choices in QS
					 (i.e. pivot sequences)
Key random var: for sigma in Omega,
	C(sigma) = # of comparisons between two input elements made by QS

Lemma: running time of QS dominated by comparisons
		i.e. there exists constant c s.t. f.a. sigma in Omega,
		     RT(sigma) <= c * C(sigma)

Need to prove E[C] = O(n logn)




Building Blocks
=======================================
Note: can't apply Master Method [random, unbalanced subproblems]
[ A = fixed input array ]
Notation: z_i = ith smallest element of A
For sigma in Omega, indices i < j, let
	X_ij(sigma) = # of times z_i, z_j get compared in QS


Given two elements in array, elements could get compared 0 or 1 times
In partition: Pivot element is compared to each other array element once
Can't get more than 1 because the pivot is excluded in the recursion.
Thus: each X_ij is an "indicator" (0,1) random variable



C(sigma) = # of comparisons between input elements
X_ij(sigma) = # of comparisons between z_i and z_j
Thus: f.a. sigma, C(sigma) = sum(i=1,n-1){ sum(j=i+1,n){ X_ij(sigma) }}
By LINEXP:
	E[C] = sum(i=1,n-1){ sum(j=i+1,n){ E[X_ij] }}
	LHS is complicated, RHS is simple.
Since E[X_ij] = 0 * Pr[X_ij=0] + 1 * Pr[X_ij=1] = Pr[X_ij=1]
Thus:
	E[C] = sum(i=1,n-1){ sum(j=i+1,n){ Pr[z_i, z_j get compared] }} (*)




General Decomposition Principle
=======================================
1. Identify random variable Y that you care about
2. Express Y as sum of indicator random variables:
		C = sum(l=1,m){ X_l }
3. Apply linearity of expectation:
		E[Y] = sum(l=1,m){ Pr[X_l=1] }








/////////////////////////////////////////////////////////////////////
6.2 - Analysis II - The Key Insight
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Key claim: f.a. i < j, Pr[X_ij=1] = 2 / (j-i+1)



Proof of Key Claim
=======================================
Fix z_i, z_j with i < j
Consider set z_i, z_i+1,...,z_j-1, z_j
Inductively: as long as none of these are chosen as a pivot, all are passed
			 to the same recursive call
Consider the first among z_i,z_i+1,...z_j-1,z_j that gets chosen as a pivot
1. if z_i or z_j gets chosen first, they get compared
2. if one of inside range gets chosen first, they do not get compared


Since pivot chosen uniformly at random, total outcomes j - i + 1
Pr[X_ij] = 2 / (j-i+1)
QED!


E[C] = sum(i=1,n-1){ sum(j=i+1,n){ 2 / (j-i+1) }}   (*)





/////////////////////////////////////////////////////////////////////
6.3 - Analysis III - Final calculations
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

E[C] = 2 * sum(i=1,n-1){ sum(j=i+1,n){ 1 / (j-i+1) }}
		   | ---- TH(n^2) terms ------------------|
		   
How big can first sum be?

For each fixed i, the inner sum is
	sum(j=i+1,n){ 1/(j-i+1) } = 1/2 + 1/3 + 1/4 +...

So: E[C] <= 2 * n * sum(k=2,n){ 1/k }



Claim: sum(k=2,n){ 1/k } <= ln n.
Pictoral proof: 
/Users/andrewsouthpaw/Documents/Programming/Algorithms Part I/Notes/Images/1.png



So: E[C] <= 2n ln n


























=====================================================================
Section 7 - Probability Review
=====================================================================


/////////////////////////////////////////////////////////////////////
7.1 - Probability Review - Part I
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Sample space
=======================================
Sample space Omega = "all possible outcomes"
[in algorithms, Omega is usually finite]
each outcome i in Omega has probability p(i) >= 0
Constraint: sum(i in Omega)[ p(i) ] = 1



Events
=======================================
Event is a subset S <= Omega
Probability of event S is sum(i in S)[ p(i) ]
Ex. event "sum of two dice is 7" is 1/6




Random variables
=======================================
A random variable X is real-valued function
	X: Omega -> R

Examples:
	- running time of a coin flip
	- sum of two dice
	- size of subarray passed to 1st recursive call



Expectation
=======================================
Let X: Omega -> R be a random variable
The expectaiton E[X] of X = average value of X
	= sum(i in Omega) [ X(i)*p(i) ]

Expectation of sum of two dice: 7
Expectation of size of subarray passed to first recursive call of QS: n/2
	Let X = subarray size
	E[X] = 1/n * 0 + 1/n * 1 + 1/n * 2 + ... + 1/n (n - 1)
		 = (n-1)/2



Linearity of Expectation
=======================================
Claim: Let X_1,...,X_n be random variables defined on Omega
Then: 
		E[ sum(i=1,n)[ X_i ] ] = sum(i=1,n)[ E[X_i] ]
Crucially: holds even when X_i's are not independent
(Would fail if replacing sums with products)

"Expectation of a sum is the sum of the expectations"

Ex#1: if X_1, X_2 = the two dice
	  E[X_j] = 1/6(1+2+3+4+5+6) = 3.5
	  By LIN EXP: E[X_1 + X_2] = E[X_1] + E[X_2] = 7


Proof of Linearity of Expectation
=======================================
sum(j=1,n)[ E[X_j] ] = sum(j=1,n)[ sum(i in Omega)[ X_j(i)p(i) ]]
					 = sum(i in Omega)[ sum(j=1,n)[ X_j(i)p(i) ]]
					 = sum(i in Omega)[ p(i) ] * sum(j=1,n)[ X_j(i) ]
					 = E[ sum(j=1,n)[ X_j ]]
QED




Example: Load Balancing
=======================================
Problem: assign n processes to n servers
Proposed solution: assign each process to a random server.
Question: what is expected number of processes assigned to a server?

Sample space Omega:
	all n^n assignments of processes to servers, each equally likely

Let Y = total number of processes assigned to first server
Goal: compute E[Y]

Let X_j = { 1 if jth process assigned to first server
			0 otherwise
	called "indicator random variables"

Note: Y = sum(j=1,n)[X_j]

E[Y] = E[ sum(j=1,n)[ X_j ] ]
	 = sum(j=1,n){E[X_j]}
	 = sum(j=1,n){ Pr[X_j=0] * 0 + Pr[X_j=1] * 1 }
	                                1/n
	 = sum(j=1,n){ 1/n }
	 = 1




	


/////////////////////////////////////////////////////////////////////
7.2 - Probability Review - Part II
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\



Conditional Probability
=======================================

Let X, Y <= OM be events
Pr[X|Y] = Pr[X n Y] / Pr[Y]
("X given Y")




Independence
=======================================
Events are independent iff
	Pr[X n Y] = Pr[X] * Pr[Y]
This holds <=> Pr[X|Y] = Pr[X] <=> Pr[Y|X] = Pr[Y]


Independence (of random variables)
=======================================
Random variables A, B are independent <=> the events Pr[A=a], Pr[B=b]
are independent for all a,b [<=> Pr[A=a and B=b] = Pr[A=a]*Pr[B=b]

If A,B are independent, then E[A*B] = E[A]*E[B]
































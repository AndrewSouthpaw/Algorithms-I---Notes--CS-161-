Week 2



=====================================================================
Section 4 - The Master Method
=====================================================================


/////////////////////////////////////////////////////////////////////
4.1 - Motivation
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


T(n) = max number ops algorithm needs
Express T(n) in terms of running time of recursive calls

Recall integer multiplication

Base case: T(1) <= a constant
For all n > 1: T(n) <= 4T(n/2) + O(n) 
               (recursive work)  (work here)

Algorithm #2 (Gauss): recursively compute ac, bd, (a+b)(c+d)
recall ad + bc = (3) - (1) - (2)

New recurrence: 
	Base case: T(1) <= a constant
	For n > 1: T(n) <= 3T(n/2) + O(n)
	



/////////////////////////////////////////////////////////////////////
4.2 - Formal statement
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Cool feature: a "black box" for solving recurrences.
Assumption: all subproblems have equal size


Recurrence format
=======================================

1) Base case: T(n) <= a constant for sufficiently small n
2) For all larger n: 
		T(n) <= a*T(n/b) + O(n^d)
		
		where a = number of recursive calls ( >=1)
		      b = factor by which input size shrinks ( >1)
		      d = exponent in running time of "combine step" ( >= 0)
			[a,b,d independent of n]




The Master Method
=======================================

Upper bounded by...

T(n) = {	O(n^d * log(n))		if a = b^d  Case 1
			O(n^d)				if a < b^d  Case 2
			O(n^log(base b)(a))	if a > b^d  Case 3


In case 1, log base doesn't matter. log(base e) and log(base 2) differ
by constant factor.

In case 3, log base matters because it's in the exponent.

In case 2, running time governed by work done outside recursive calls



	
/////////////////////////////////////////////////////////////////////
4.3 - Examples
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Example 1: MergeSort
=======================================

a = 2
b = 2
d = 1

Thus, Case 1: a = b^d
Thus, T(n) <= O(n^d * log(n)) = O(nlog(n))



Example 2: Binary search in sorted array
=======================================

a = 1
b = 2
d = 0

Thus, Case 1:  T(n) <= O(log(n))




Example 3: Integer Multiplication (4 subproblems)
=================================================

a = 4
b = 2
d = 1   <-- additions, padding by 0, thus linear time

Case 3:  T(n) <= O(n^log(base b)(a)) = O(n^log(base 2)(4)) = O(n^2)




Example 4: Integer Multiplication (Gauss's trick)
=================================================

a = 3
b = 2
d = 1

Case 3: T(n) <= O(n^log(base 2)(3)) = O(n^1.59)
Definitely better than n^2...




Example 5: Strassen's Matrix Multiplication
===========================================

Naive approach leads to 8 subproblems
With Strassen's, you get 7.

a = 7
b = 2
d = 2 <-- linear in matrix size, quadratic number of entries, n^2

Case 3: T(n) <= O(n^log(base 2)(7)) = O(n^2.81)
That's a win compared with n^3




Example 6: Ficticious recurrence
=======================================

T(n) <= 2T(n/2) + O(n^2)

=>  a = 2
	b = 2
	d = 2

Case 2:  T(n) <= O(n^2)





/////////////////////////////////////////////////////////////////////
4.4 - Proof of Master Method
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Preamble
=======================================

Assume: recurrence is
			i. T(1) <= c
			ii. T(n) <= aT(n/b) + cn^d
			for some constant c
		and...
			n is a power of b
			(general case similar, but more tedious)



Analysis (similar to MergeSort) using recursion tree
====================================================

At level j = 0,1,...log(base b)(n), 
	there are a^j subproblems each of size n/(b^j)
	
Recursion tree:

	level 0				(input size n)
					/        |            \  (a branches)
	level 1      (n/b)      (n/b)       (n/b)
						...
	level logb(n)	......................... base cases size 1

Work at level j [ignoring work in recursive calls]:
	<= a^j              *     c * [n/(b^j)]^d  
	  # of lvl-j subproblems     size of each level-j subproblem
							 work per level-j subproblem (by ii. in assumption)

	=  cn^d * [a/(b^d)]^j


Total work:
	Summing over all levels
	total work <= cn^d * sum(j=0,log(base b)(n))[a/(b^d)]^j
		call it (*)



/////////////////////////////////////////////////////////////////////
4.5 - Interpretation for 3 Cases
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Interpretation:
	ratio of a/b^d is key to running time
	
	a = rate of subproblem proliferation (RSP)
	b^d = rate of work shrinkage per subproblem (RWS)
	

Tug of war between forces of good (RWS) and evil (RSP)!


Statements:
	- If RSP < RWS, then amount of work is descreasing with recur level j
			True.
	- If RSP > RWS, then amount of work is increasing with recur level j
			True.
	- No conclusions can be drawn about how amount of work varies with recur
	  level j unless RSP and RWS are equal
	  		False, given i and ii
	- If RSP and RWS are equal, amount of work is same at every recur level j



Intuition for 3 cases
=======================================

Upper bound for level j:  cn^d * (a/b^d)^j

1) RSP = RWS => same amount of work each level
	[expect O(n^d*log(n))]

2) RSP < RWS => less work each level => most work at root
	[might expect O(n^d)]

3) RSP > RWS => more work each level => most work at leaves
	[might expect O(# leaves)]


Case 3: how do we get n^log(base b)(a)?





/////////////////////////////////////////////////////////////////////
4.6 - Proof II
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\














Week 2



=====================================================================
Section 4 - The Master Method
=====================================================================


/////////////////////////////////////////////////////////////////////
4.1 - Motivation
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


T(n) = max number ops algorithm needs
Express T(n) in terms of running time of recursive calls

Recall integer multiplication

Base case: T(1) <= a constant
For all n > 1: T(n) <= 4T(n/2) + O(n) 
               (recursive work)  (work here)

Algorithm #2 (Gauss): recursively compute ac, bd, (a+b)(c+d)
recall ad + bc = (3) - (1) - (2)

New recurrence: 
	Base case: T(1) <= a constant
	For n > 1: T(n) <= 3T(n/2) + O(n)
	



/////////////////////////////////////////////////////////////////////
4.2 - Formal statement
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Cool feature: a "black box" for solving recurrences.
Assumption: all subproblems have equal size


Recurrence format
=======================================

1) Base case: T(n) <= a constant for sufficiently small n
2) For all larger n: 
		T(n) <= a*T(n/b) + O(n^d)
		
		where a = number of recursive calls ( >=1)
		      b = factor by which input size shrinks ( >1)
		      d = exponent in running time of "combine step" ( >= 0)
			[a,b,d independent of n]




The Master Method
=======================================

Upper bounded by...

T(n) = {	O(n^d * log(n))		if a = b^d  Case 1
			O(n^d)				if a < b^d  Case 2
			O(n^log(base b)(a))	if a > b^d  Case 3


In case 1, log base doesn't matter. log(base e) and log(base 2) differ
by constant factor.

In case 3, log base matters because it's in the exponent.

In case 2, running time governed by work done outside recursive calls



	
/////////////////////////////////////////////////////////////////////
4.3 - Examples
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Example 1: MergeSort
=======================================

a = 2
b = 2
d = 1

Thus, Case 1: a = b^d
Thus, T(n) <= O(n^d * log(n)) = O(nlog(n))



Example 2: Binary search in sorted array
=======================================

a = 1
b = 2
d = 0

Thus, Case 1:  T(n) <= O(log(n))




Example 3: Integer Multiplication (4 subproblems)
=================================================

a = 4
b = 2
d = 1   <-- additions, padding by 0, thus linear time

Case 3:  T(n) <= O(n^log(base b)(a)) = O(n^log(base 2)(4)) = O(n^2)




Example 4: Integer Multiplication (Gauss's trick)
=================================================

a = 3
b = 2
d = 1

Case 3: T(n) <= O(n^log(base 2)(3)) = O(n^1.59)
Definitely better than n^2...




Example 5: Strassen's Matrix Multiplication
===========================================

Naive approach leads to 8 subproblems
With Strassen's, you get 7.

a = 7
b = 2
d = 2 <-- linear in matrix size, quadratic number of entries, n^2

Case 3: T(n) <= O(n^log(base 2)(7)) = O(n^2.81)
That's a win compared with n^3




Example 6: Ficticious recurrence
=======================================

T(n) <= 2T(n/2) + O(n^2)

=>  a = 2
	b = 2
	d = 2

Case 2:  T(n) <= O(n^2)





/////////////////////////////////////////////////////////////////////
4.4 - Proof of Master Method
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Preamble
=======================================

Assume: recurrence is
			i. T(1) <= c
			ii. T(n) <= aT(n/b) + cn^d
			for some constant c
		and...
			n is a power of b
			(general case similar, but more tedious)



Analysis (similar to MergeSort) using recursion tree
====================================================

At level j = 0,1,...log(base b)(n), 
	there are a^j subproblems each of size n/(b^j)
	
Recursion tree:

	level 0				(input size n)
					/        |            \  (a branches)
	level 1      (n/b)      (n/b)       (n/b)
						...
	level logb(n)	......................... base cases size 1

Work at level j [ignoring work in recursive calls]:
	<= a^j              *     c * [n/(b^j)]^d  
	  # of lvl-j subproblems     size of each level-j subproblem
							 work per level-j subproblem (by ii. in assumption)

	=  cn^d * [a/(b^d)]^j


Total work:
	Summing over all levels
	total work <= cn^d * sum(j=0,log(base b)(n))[a/(b^d)]^j
		call it (*)



/////////////////////////////////////////////////////////////////////
4.5 - Interpretation for 3 Cases
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Interpretation:
	ratio of a/b^d is key to running time
	
	a = rate of subproblem proliferation (RSP)
	b^d = rate of work shrinkage per subproblem (RWS)
	

Tug of war between forces of good (RWS) and evil (RSP)!


Statements:
	- If RSP < RWS, then amount of work is descreasing with recur level j
			True.
	- If RSP > RWS, then amount of work is increasing with recur level j
			True.
	- No conclusions can be drawn about how amount of work varies with recur
	  level j unless RSP and RWS are equal
	  		False, given i and ii
	- If RSP and RWS are equal, amount of work is same at every recur level j



Intuition for 3 cases
=======================================

Upper bound for level j:  cn^d * (a/b^d)^j

1) RSP = RWS => same amount of work each level
	[expect O(n^d*log(n))]

2) RSP < RWS => less work each level => most work at root
	[might expect O(n^d)]

3) RSP > RWS => more work each level => most work at leaves
	[might expect O(# leaves)]


Case 3: how do we get n^log(base b)(a)?





/////////////////////////////////////////////////////////////////////
4.6 - Proof II
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Total work <= cn^d * sum(j=0,log(base b)(n))[ (a/b^d)^j ]  (*)

Case 1, a = b^d:
	(a/b^d)^j reduces to 1
	1 summed to log(base b)(n) times = log(base b)(n) + 1

	(*) = cn^d(log(base b)(n) + 1)
		= O(n^d log(n))

[end case 1]



Basic Sums Fact
=======================================

For r != 1, we have 
	1 + r + r^2 + r^3 +...+ r^k = (r^(k+1) - 1)/(r - 1)

Proof: by induction (you check)
Check out canonical cases of r = 1/2, r = 2

Upshot:
	1) if r < 1 is constant, bounded by <= 1 / (1-r) = a constant
	   independent of k. 1st term of sum dominates.
	2) if r > 1 is constant, RHS is <= r^k * (1 + 1/(r-1))
	   last term of sum dominates



Case 2
=======================================

If a < b^d...

then ratio a/b^d := r < 1   =>  <= a constant independent of n
thus (*) evals to...
	= O(n^d)

total work dominated by top level



Case 3
=======================================

If a > b^d...

then ratio a/b^d := r > 1 =>  <= constant * largest term of sum
take biggest term of sum, which is log(base b)(n)

then (*) = O(n^d * (a/b^d)^log(base b)(n))

Note: b^(-d*log(base b)(n)) = (b^log(base b)n)^-d = n^-d
So: (*) = O(a ^ log(base b)n) , which is number of leaves of recursion tree

We get a children each level, goes on until n reaches 1 by dividing by b,
or log(base b)n

Believe it or not...
	a^log(base b)n  = n^log(base b)a
	to show, take log(base b) of both sides.
	[since (log(base b)n)(log(base b)a) = (log(base b)a)(log(base b)n)]
	
	LHS is more intuitive, RHS is simpler to apply.

[end case 3]

QED!




=====================================================================
Section 5 - Quicksort - Algorithm
=====================================================================



/////////////////////////////////////////////////////////////////////
5.1 - Quicksort overview
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


 Input: array of n numbers, unsorted [3 8 2 5 1 4 7 6]
 Output: same numbers, sorted in increasing order
 Assume: all entries distinct
 Exercise: extend QuickSort to handle duplicate entries
 
 
Partitioning around a pivot
======================================
 
Key idea: partition array around pivot element
- pick element of array
- rearrange element so:
	left of pivot => less than pivot
	right of pivot => greater than pivot
Note: puts pivot in its "rightful position"


Two cool facts about partition
=======================================
1) linear time, no extra memory (only doing swaps)
2) reduces problem size 


High-level description
=======================================

QuickSort(array A, length n) {
	if n = 1 return
	p = choosePivot(A,n)
	partition A around p
	recursively sort 1st part
	recursively sort 2nd part
}





/////////////////////////////////////////////////////////////////////
5.2 - Partition Subroutine
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


In-Place Implementation
=======================================

Assume: pivot = 1st element of array.
Could swap pivot element to be 1st and still use this implementation

High-level idea:
	[p][ already partitioned ][unpartitioned]
	- single scan through array
	- invariant: everything we've looked at is partitioned

Two boundaries: between partitioned and unpartitioned (j)
			    between < p and > p (i)

At each step, we want to advance j.
When you encounter element < pivot, swap with element after i, and advance i
Terminate with swapping pivot and element to the left of i



Pseudocode
=======================================

partition(A, l, r) {
	p = A[l]
	i = l+1
	for j = l+1 to r
		if A[j] < p   // if A[j] > p, do nothing
			swap A[j] and A[i]  // doesn't avoid redundant swaps when no elements
			                    // exist bigger than the pivot
			i++
	swap A[l] and A[i-1]
}



Running time
=======================================

T(n) <= O(n), where n = r - l + 1 is length of input (sub)array
Reason: O(1) work per array entry
Also: clearly works in place (repeated swaps)




Correctness
=======================================

Claim: the for loop maintains the invariant: [p][ < p ][ > p][ ?    ]

1) A[l+1],...,A[i-1] are all less than pivot
2) A[i],...,A[j-1] are all greater than pivot
Exercise: check this by induction

Upshot: at end of for loop and you swap the pivot, the pivot is
"in rightful place"

QED






/////////////////////////////////////////////////////////////////////
5.3 - Correctness of Quicksort [optional]
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Induction Review
=======================================

Let P(n) = assertion parameterized by positive integers n.

For us: 
	P(n) is "QuickSort correctly sorts every input array of length n"

How to prove P(n) for all n >= 1 by induction:
	1) [base case] directly prove the P(1) holds
	2) [inductive step] for every n >= 2, prove that:
		if P(k) holds for all k < n, then P(n) holds as well
		|-inductive hypothesis ----|




Correctness of QuickSort
=======================================

P(n) = "QuickSort correctly sorts every input array of length n"

Claim: 
	P(n) holds for every n >= 1, regardless of pivot selection

Proof by induction:
	1) [base case] every input array of length 1 is already sorted
		QuickSort returns the input array, which is correct. ( P(1) holds)
	2) [inductive step] Fix n >= 2. Fix some input array A of length n.
		Show: if P(k) holds for all k < n, then P(n) holds as well
		
Recall:
	QuickSort partitions A around some pivot p.  [ < p ][ p ][ > p ]
Note:
	pivot winds up in correct position.

Let k_1, k_2 = lengths of 1st, 2nd parts of partitioned array.
	k_1, k_2 < n

By inductive hypothesis: 1st, 2nd parts get sorted correctly by recursive calls.
	(using P(k_1), P(k_2), which are correct by inductive hypothesis)

So: after recursive calls, entire array correctly sorted.






































Week 2



=====================================================================
Section 4 - The Master Method
=====================================================================


/////////////////////////////////////////////////////////////////////
4.1 - Motivation
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


T(n) = max number ops algorithm needs
Express T(n) in terms of running time of recursive calls

Recall integer multiplication

Base case: T(1) <= a constant
For all n > 1: T(n) <= 4T(n/2) + O(n) 
               (recursive work)  (work here)

Algorithm #2 (Gauss): recursively compute ac, bd, (a+b)(c+d)
recall ad + bc = (3) - (1) - (2)

New recurrence: 
	Base case: T(1) <= a constant
	For n > 1: T(n) <= 3T(n/2) + O(n)
	



/////////////////////////////////////////////////////////////////////
4.2 - Formal statement
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Cool feature: a "black box" for solving recurrences.
Assumption: all subproblems have equal size


Recurrence format
=======================================

1) Base case: T(n) <= a constant for sufficiently small n
2) For all larger n: 
		T(n) <= a*T(n/b) + O(n^d)
		
		where a = number of recursive calls ( >=1)
		      b = factor by which input size shrinks ( >1)
		      d = exponent in running time of "combine step" ( >= 0)
			[a,b,d independent of n]




The Master Method
=======================================

Upper bounded by...

T(n) = {	O(n^d * log(n))		if a = b^d  Case 1
			O(n^d)				if a < b^d  Case 2
			O(n^log(base b)(a))	if a > b^d  Case 3


In case 1, log base doesn't matter. log(base e) and log(base 2) differ
by constant factor.

In case 3, log base matters because it's in the exponent.

In case 2, running time governed by work done outside recursive calls



	
/////////////////////////////////////////////////////////////////////
4.3 - Examples
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Example 1: MergeSort
=======================================

a = 2
b = 2
d = 1

Thus, Case 1: a = b^d
Thus, T(n) <= O(n^d * log(n)) = O(nlog(n))



Example 2: Binary search in sorted array
=======================================

a = 1
b = 2
d = 0

Thus, Case 1:  T(n) <= O(log(n))




Example 3: Integer Multiplication (4 subproblems)
=================================================

a = 4
b = 2
d = 1   <-- additions, padding by 0, thus linear time

Case 3:  T(n) <= O(n^log(base b)(a)) = O(n^log(base 2)(4)) = O(n^2)




Example 4: Integer Multiplication (Gauss's trick)
=================================================

a = 3
b = 2
d = 1

Case 3: T(n) <= O(n^log(base 2)(3)) = O(n^1.59)
Definitely better than n^2...




Example 5: Strassen's Matrix Multiplication
===========================================

Naive approach leads to 8 subproblems
With Strassen's, you get 7.

a = 7
b = 2
d = 2 <-- linear in matrix size, quadratic number of entries, n^2

Case 3: T(n) <= O(n^log(base 2)(7)) = O(n^2.81)
That's a win compared with n^3




Example 6: Ficticious recurrence
=======================================

T(n) <= 2T(n/2) + O(n^2)

=>  a = 2
	b = 2
	d = 2

Case 2:  T(n) <= O(n^2)

































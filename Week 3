Week 3




=====================================================================
Section 8 - Linear-Time Selection
=====================================================================


/////////////////////////////////////////////////////////////////////
8.1 - Randomized Selection
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Input: array A with n distinct numbers and a number i
Output: ith order statistic
Example: median: i = (n+1)/2 for n odd, n/2 for n even



Reduction to sorting
=======================================
1. apply MergeSort
2. return ith element of sorted array
Problem: can't sort any faster than nlog(n)
Next: O(n) time (randomized) by modifying QuickSort
Optional: O(n) time deterministic algorithm
			- pivot = "median of medians" (not as practical)



Using QuickSort
=======================================
Partition around pivot. To get ith element array, you only need to recurse
on one subproblem.
	E.g. if looking for 5th order statistic in input array on length 10,
	     and pivot ends in third position, you'd recurse on right side of
	     pivot and look for 2nd order statistic


Randomized selection
=======================================
RSelect (array A, length n, order statistic i)
	0. if n = 1 return A[1]
	1. choose pivot p from A uniformly at random
	2. partition A around p
	   let j = order statistic of p (new index of p)
	3. if j = i return p
	4. if j > i return RSelect(1st part of A, j-1, i)
	5. (if j < i) return RSelect(2nd part of A, n-j, i-j)



Properties of RSelect
=======================================
Claim: RSelect is correct (guaranteed to output ith order statistic)
Proof: by induction [like optional QuickSort video]
Running Time: depends on "quality" of chosen pivots

If pivot always chosen in worst possible way: TH(n^2)

Key: find pivot giving "balanced" split
Best pivot: the median (but, this is circular reasoning...)
	=> would get recurrence T(n) <= T(n/2) + O(n)
	=> T(n) = O(n) [case 2 of Master Method]
(this is just a sanity check... not actually feasible)

Hope: random pivots will be "pretty good" "often enough"



RSelect Theorem
=======================================
For every input of array of length n, the average running time
of RSelect is O(n)
- holds for every input
- "average" over random pivot choices






/////////////////////////////////////////////////////////////////////
8.1 - Randomized Selection - Analysis
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\



Proof I: Tracking Progress via Phases
=======================================
Note: RSelect uses <= cn operations outside recursive call [c >0] in partitioning
Notation: RSelect is in phase j 
		  if current array size b/n (3/4)^(j+1)*n and (3/4)^j*n
		  X_j = number of recursive calls during phase j
Why? we want to cleanly talk about the progress. X_j will help us with that.


running time RSelect <= sum(phases j){X_j * c * (3/4)^j*n}
												^-- <= array size in j
											^-- work per subproblem
									  ^-- # of phase-j subproblems





Proof II: Reduction to Coin Flipping
=======================================
X_j = # of recursive calls during phase j b/n size (3/4)^(j+1)*n and (3/4)^j*n
Note: if RSelect chooses pivot giving 25-75 split (or better), 
	  then current phase ends. New subarray length <= 75% old length
Recall: Probability of this is 50%. Like a coin flip.
So: E[X_j] <= expected number of times you need to flip coin to see "heads"



Proof III: Coin Flipping Analysis
=======================================
Let N = number of coin flips until you get heads
		(a "geometric random variable")
E[N] = 1 + 1/2*E[N]
	   ^-- 1st coin flip
	       ^-- probability of tails
	           ^-- # of further coin flips needed in this case
Solution: E[N] = 2.



Putting it all together
=======================================
running time RSelect <= E[cn * sum(phases j){(3/4)^j * X_j]}
LIN EXP=>
					  = cn * sum(phase j){(3/4)^j*E[X_j]}
					  = 2cn * sum(phase j){(3/4)^j}
					                       ^-- from Master Method, geometric sum,
					                           <= 1/(1-n) = 1/(1-3/4) = 4
					<= 8cn
					
Thus running time = O(n)
QED







=====================================================================
Section 9 - Graphs and Contraction Algorithm
=====================================================================


/////////////////////////////////////////////////////////////////////
9.1 - Graphs and Minimum Cuts
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

Graphs
=======================================
Two ingredients
	- vertices aka nodes (V) - objects
	- edges (E) = pairs of vertices
		- can be undirected [unordered pair]
		      or directed [tail, head of ordered pair, aka arcs]

Examples: road netowrks, the web, social networks, precedence constraints




Cuts of Graphs
=======================================
A cut of a graph (V,E) is partition of V into two non-empty sets A,B
The "crossing edges" of cut (A,B) are those with:
	- one endpoint in each (A,B) [undirected]
	- tail in A, head in B [directed]

For graph with n vertices, roughly 2^n cuts  (technically 2^n - 2)




Minimum cut problem
=======================================
Input: undirected graph G = (V,E)
	   [parallel edges allowed]
	   [see other video for representation of input]
Goal: compute cut with fewest number of crossing edges ("min cut")




Applications
=======================================
- Identify weaknesses (fewer links) of network (e.g. transportation network)
- Community detection (tightly-knit pockets of people, highly connected within
                       themselves, weakly connected to everyone else)
- Image segmentation
		- input = graph of pixels
		- use edge weights [(u,v) has large weight <=> "expect" u,v to come from
		                    same object]
		- hope: repeated min cuts identify primary objects in picture













/////////////////////////////////////////////////////////////////////
9.2 - Graph Representations
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

For undirected graph, n vertices, no parallel edges, connected, 
min edges: n-1
max edges: n(n-1)/2  (n choose 2)



Sparse v. Dense graphs
=======================================
n = # vertices, m = # edges
In most applications, m is OM(n) and O(n^2)
- in "sparse graph," m is O(n) or close to it
- in "dense graph," m is closer to TH(n^2)




Adjacency Matrix
=======================================
Represent G by a nxn O-1 matrix A, where
	A_ij = 1 <=> G has i-j edge  (i)----(j)
Variants
	- A_ij = # of i-j edges (if parallel edges)
	- A_ij = weight of i-j edge (if any)
	- A_ij = { +1 if (i)--->(j)
	           -1 if (i)<---(j) }

Space requirement: TH(n^2)
Fine for dense graph, wasteful for sparse graph, impossible for massive graphs




Adjacency Lists
=======================================
Ingredients
	- array (or list) of vertices	[ TH(n) ]
	- array (or list) of edges		[ TH(m) ]
	- each edge points to its endpoints		[ TH(m) ]
	- each vertex points to edges incident (1-hop) on it
									[ TH(m) ] 1-to-1 correspondence with #3

Space requirement: TH(m + n) or TH(max{m,n})


Which is better? Depends on graph density and operations needed
We'll focus on Adjacency Lists 
	(good for graph search, usually dealing with MASSIVE graphs)
	e.g. web links ~10^10. For adjacency matrix, that'd be 10^20.
	     (10^80 ~= atoms in known universe)






















































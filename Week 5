Week 5







=====================================================================
Section 11 - Dijkstra's Shortest-Path Algorithm
=====================================================================


/////////////////////////////////////////////////////////////////////
11.1 - Dijkstra's Shortest Path Algorithm
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\



Single-Source shortest paths
=======================================
Input: directed graph (G=(V,E)). m = |E|, n = |V|
- each edge has nonnegative length l_e
- source vertex s
Output: for each v in V, compute L(v) = length of shortest s-v path in G
Assumptions:
	1. f.a. v in V, t.e. an s -> v path
	2. l_e >= 0 f.a. e in E


BFS only computes shortest path if l_e = 1 for every edge e
Question: why not replace each edge e by directed path of l_e unit length edges
Answer: blows up graph too much. Lengths could be 100, 1000, etc.
		Too wasteful.







Dijkstra's Algorithm
=======================================
Initialize:
	- X = {S} // vertices processed so far
	- A[S] = 0	// shortest path distances
	- B[S] = empty path 	// computed shortest path
							// not actually used in implementation, for 
							// learning

Main Loop
	- while X != V:		// grow X by one node into world unexplored
		- among all edges (v,w) in E with v in X, w not in X, pick
		  pick one that minimizes A[v] + l_vw	(Dijkstra's greedy criterion)
		  	// call it (v*,w*)
		  	// looking at edges with tail in known sphere and head in unknown
		- add w* to X
		- set A[w*] = A[v*] + l_v*w*
		- set B[w*] = B[w*] u (v*,w*)







/////////////////////////////////////////////////////////////////////
11.2 - Example
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Sucking nodes into the world of explored nodes, and then looking at all
subsequent nodes and their "greedy score"


Non-Example -- with negative lengths
=======================================
Question: why not reduce computing shortest paths with negative edge lengths
to same problem with nonnegative edge lengths (by adding large constant to
edge lengths)
Problem: doesn't preserve shortest paths! 
		path one: 1, -5	 (shortest)		=> 6, 0
		path two: -2					=> 3	(shortest)
		
Also: Dijkstra's algorithm incorrect, getting messed up by the greedy score
That's the trouble with being greedy... :)












/////////////////////////////////////////////////////////////////////
11.3 - Correctness Proof
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Theorem: for every directed graph with nonnegative edge lengths, Dijkstra's
algorithm computes all shortest-path distances
	A[v] = L(v) f.a. v in V
	        ^-- true shortest path

Proof: by induction on number of iterations
Base case: A[s] = L(s) = 0
Inductive step:
Inductive hypothesis: all previous iterations correct
	f.a. v in X, A[v] = L(v) and
				 B[v] is true shortest s-v path in G

In current iteration:
	We pick edge (v*,w*) and add w* to X
	We set B[w*] = B[v*] u (v*,w*)
L(v*)+l_v*w*--^		^-- has length by L(v*)
	Also: A[w*] = A[v*] + l_v*w* = L(v*) + l_v*w*

Upshot:
	in current iteration, we set:
	1. A[w*] = L(v*) + l_v*w*	// output of path to w* is true shortest path 
								// to v* + length v* to w*
	2. B[w*] = bonafide path from s to w* with length above

To finish proof: show that every s-w* path has
	length >= L(v*) + l_v*w*  (if so, our path is the shortest)
	
So let P = any path s->w* path
Every path s->w* has to cross frontier at least once
	s in X, w* not in x. "cross the frontier"
and so has form  s -> y -> (frontier) -> z -> w*
				prefix    direct edge     suffix

	l_yz
	l_zw* >= 0 (no negative edges)
	l_sy is some path to y, therefore >= length of shortest s-y path
										 (inductive hypothesis)
		 = L(y) = A[y] by inductive hypothesis

Total length of path P: >= A[y] + l_yz + 0
=> by Dijkstra's greedy criterion, A[v*] + l_v*w* <= A[y] + l_yz
  length of our path <= length of competing path P
  because we choose the path with the best greedy score















/////////////////////////////////////////////////////////////////////
11.4 - Implementation and Running Time of Dijkstra's
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Naive implementation running time: TH(mn)
	- (n-1) iterations of while loop
	- linear scan through all edges
	- TH(m) work per iteration, TH(1) work per edge


Heap Operations
=======================================
Question: we're doing minimum scans repeatedly, is there a way to keep it 
around?
Heap property: at every node, key <= children's keys
- perfectly balanced binary tree (height ~= log(base2)(n) )
- extractMin by swapping up last leaf, bubbling down
- insert via bubbling up
- delete from middle (bubble up or down as needed)



Invariants
=======================================
Invariant 1: elements in heap = vertices of V-X
Invariant 2: for v n.i. X, key [v] = smallest Dijkstra greedy score of an edge
									 (u,v) in E with u in X
									 (+INF if no such edge exist)

Point: by invariants, extractMin yields correct vertex w* to add to X next





Maintaining the invariants
=======================================
When you add a new vertex into X, you'll need to update the keys that go out
from w

when w extracted from heap
- for each edge (w,v) in E:
	- if v in V-X (i.e. in heap)
		// key update
		- delete v from heap
		- recompute key[v] = min{key[v], A[w]+l_wv}
		- reinsert v into heap


Run-time: dominated by heap operations. (O(log n) each)
- (n-1) extractMins
- each edge (v,w) triggers at most one Delete/Insert combo 
	(if v added to X first)
  (as opposed to vertex-centric view)
So: # of heap operations is O(n+m) = O(m)  (since m dominates n, weakly connected)
So: running time = O(m log n)   (like sorting)




















=====================================================================
Section 12 - Heaps
=====================================================================
















/////////////////////////////////////////////////////////////////////
12.1 - Data structures overview
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Purpose: organize data so it can be accessed quickly and usefully
Ex: lists, stacks, queues, heaps, search trees, hash tables, bloom filters,
	union-find, etc.
different data structures support different sets of operations
Rule of thumb: choose "minimal" data structure that supports all the
			   operations you need, and no more.


Data structure levels
	Level 0 = "what's a data structure?"
	Level 1 = cocktail party awareness
	Level 2 = comfortable using them as client, good sense of what is suitable
	Level 3 = understands how they are implemented














/////////////////////////////////////////////////////////////////////
12.2 - Heaps - Operations and Applications
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


- a container for objects that have keys
	- employer records, network edges, events, etc.

Insert: add new object to heap
	[O(log n)]
Extract-min: remove object in heap with minimum key value
		     [ties broken arbitrarily, also possible extract-max]
	[O(log n) - n = # objects in heap]

HEAPIFY(n batched inserts in O(n) time), Delete (O(log n) time)



Application: sorting
=======================================
Canonical use of heap: fast way to do repeated minimum computations
Example: SelectionSort => O(n^2) 
HeapSort: insert elements, then extract mins => O(n log n)
		  => optimal for "comparison-based" sorting algorithm
		  => pretty damn close to quicksort



Application: Event Manager
=======================================
"Priority Queue" - synonym for heap
Example: simulation (e.g. for a video game)
	- objects = event records [action/update to occur at given time in future]
	- keys = time event scheduled to occur
	- Extract-Min => yields the next scheduled event




Application: Median Maintenance
=======================================
I give you: sequence x1,...,xn of numbers, one-by-one
You tell me: at each time step i, the median of the set
Constraint: use O(log i) time at each step i.
Solution: maintain heaps H_low : support extractMax
						 H_high: support extractMin
Maintain invariant that 1/2 smallest (largest) elements in H_low (H_high)
You check: 
	1. can maintain invariant with O(log i) work
	2. given invariant, can compute median in O(log i) work

Can maintain balance 50/50 split




Application: Speeding up Dijkstra
=======================================
- naivie implementation: O(nm) -- # loop it * linear scan for min comp
- with heads -> run time O(m log n)

































=====================================================================
Section 13 - Balanced Binary Search Tree
=====================================================================






/////////////////////////////////////////////////////////////////////
13.1 - BSTs - Operations and Applications
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\



Sorted arrays
	Operations								Running time
		Search								[ O(log n) 	]
		Select (given order stat. i)		[ O(1)		]
		Min/Max								[ O(1)		]
		Predecessor/Successor (given ptr)	[ O(1)		]
		Rank (# of keys <= given val)		[ O(log n)	]
		Output in sorted order				[ O(n)		]
		Insert/Delete						[ O(n) 		] <-- ew...



BSTs: like sorted array + faster insert/delete
	Search, select, min/max, pred/succ, rank 	=> O(log n)
	Output in sorted order => O(n)
	Insert, delete => O(log n)













/////////////////////////////////////////////////////////////////////
13.2 - BST Basics, Part I
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


- one node per key
- 3 ptrs: parent, left, right
- Search tree property: left subtree:  vals < node
						right subtree: vals > node
- can handle ties as long as convention establish, one is <= or >=.
- many possibilities for set of keys. Height from log(base2)(n) to n




/////////////////////////////////////////////////////////////////////
13.3. - BST Basics, Part II
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Minimum: 
	- start at root
	- follow left child ptrs until leaf reached
Maximum: follow right child ptrs until leaf reached

Predecessor of key k
	- easy case: if k's left subtree nonempty, return max key in left subtree
	- otherwise: follow parent ptrs until you get key < k (or "turn left")
	- need base case if predecessor doesn't exist

In-order traversal:
	- Do: T_l, then root, then T_r

Deletion:
	- Search for k
	- Easy case: k has no children
		- delete k's node from tree
	- Medium case: k's node has one child
		- "splice out" k's node (unique child assumes position held by k)
	- Hard case: k's node has two children  [ O(height) ]
		- compute k's predecessor l
			[easy: get max of left child]
		- swap k and l
		- delete k
		- at end you have valid search tree
		
Select and rank:
(store extra info at each tree node about tree itself)
Ex: size(x) = # of tree nodes in subtree rooted at x
			= size(y) + size(z) + 1

Select:
	- start at root x, with children y and z
	- let a = size(y) [a = 0 if x has no left child]
	- if a = i-1 return x's key
	- if a >= i recursively compute ith o.statistic on y
	- if a < i-1 recursively compute (i-a-1) o.statistic on z
	[O(height)]


Rank:
	[Do this]









/////////////////////////////////////////////////////////////////////
13.4 - Red-Black Trees
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

Red-black trees [Bayer '72]
See also AVL trees, splay trees, B trees



Red-Black Invariants
=======================================
1.	Each node red or black
2.	Root is black
3.	No 2 reds in a row [red node => only black children]
4.	Every root-NULL path (e.g. unsuccessful search) has same # black nodes

Every red-black tree has height <= 2 * log(base2)(n + 1)



Proof:
	If every root-NULL path has >= k nodes, then tree includes (at top)
	a balanced search tree of depth k-1
	=> size n of tree at least 2^k-1
	=> k <= log(base2)(n+1)
	Thus in red-black tree with n nodes, there is root-NULL path with at most
	log(base2)(n+1) black nodes
	By 4th invariant: every root-NULL path has <= log(base2)(n+1) black nodes
	By 3rd invariant: no two reds in a row, so at most reds can double
		<= 2*log(base2)(n+1) total nodes.	QED










/////////////////////////////////////////////////////////////////////
13.5 - Rotations
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

Key primitive: rotations. Common to all BST implementations
Locally rebalance subtrees at node in O(1) time
Left rotation:  (of parent x and right child y)
		p
		x
	A		y
	 	  B    C
	 (x<y)
	 A < x (and y)
	 C > y (and x)
	 B > x, B < y
	 
	Essentially invert relationship x, y; y becomes parent, x is child
				p
				y
			x		C
		  A   B


Right rotation: (of parent x and left child y)
		x
	y		C
   A  B
   
   
		y
	A		x
		   B  C


Property of rotation: maintains search tree property in O(1) time










/////////////////////////////////////////////////////////////////////
13.6 - Insertion in Red-Black Tree
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\


Idea: do regular insert/delete, then recolor or use rotations until
	  invariants are restored

Insert(x):
	1. insert x as usual (makes x a leaf)
	2. try coloring x red
	3. if x's parent y is black, done
	4. else y is red => y has black parent w
		Case 1: other child z of x's grandparent w is also red
						w
					z		y
						   x
				recolor y,z black and w red
			=> either restores inv. (3) or propagates double red upward
				can only happen O(log n) times
				if you reach root, recolor it black => preserves inv. (4)

		
		Case 2: let x,y be current double-red, x the deeper node. Let w = x's
		grandparent. Suppose w's other child (!= y) is NULL or black node z.
						w
					z		y
						   x   C
						 A  B

		Can eliminate double-red in O(1) time via 2-3 rotations and recolorings
		



























